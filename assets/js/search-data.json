{
  
    
        "post0": {
            "title": "Title",
            "content": "from nbdev import export2html . This is an unfinished preview. . This is from my tutoring that I do. This is a preview for study groups. . from fastai2.vision.all import * . path=untar_data(URLs.MNIST_TINY) . Path(&#39;/home/fast/.fastai/data&#39;).ls() . (#42) [Path(&#39;/home/fast/.fastai/data/imagenette-160.tgz&#39;),Path(&#39;/home/fast/.fastai/data/imagenette-160&#39;),Path(&#39;/home/fast/.fastai/data/mnist.pkl.gz&#39;),Path(&#39;/home/fast/.fastai/data/imagenette.tgz&#39;),Path(&#39;/home/fast/.fastai/data/imagenette&#39;),Path(&#39;/home/fast/.fastai/data/danbooru2018&#39;),Path(&#39;/home/fast/.fastai/data/horse2zebra&#39;),Path(&#39;/home/fast/.fastai/data/Selfie-dataset&#39;),Path(&#39;/home/fast/.fastai/data/oxford-iiit-pet&#39;),Path(&#39;/home/fast/.fastai/data/planet_tiny&#39;)...] . db=DataBlock((ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label,batch_tfms=aug_transforms(do_flip=False)) . dls=db.dataloaders(path,bs=16) . dls.show_batch() . Optimizers . SGD . This is basic setup, we will do this every time we change the code to reset all the involved variables. . x,labels = dls.one_batch() x,labels=x.cpu(),labels.cpu() m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3200,2)) #very basic pytorch model, just linear operation, no activation, so not a deep model l = nn.CrossEntropyLoss() lr=0.1 opt=SGD(m.parameters(),lr) . pred=m(x) loss=l(pred,labels) loss.backward() . class OurSGD: def __init__(self,params,lr): self.params,self.lr=params,lr def step(self): updated_params=[] for p in self.params: updated_params.append(p.add(-self.lr*p.grad)) #important part!!! return updated_params . The important part above is the p.add(-self.lr * p.grad) part. This is the essence of SGD. Notice that we are returning the updated parameters in a list. This is not done in the actual implementation, and instead everything is updated in place. Otherwise this is effectively the same as the Fastai source code. . our_sgd=OurSGD(m.parameters(),lr) . our_parameters=our_sgd.step() . Q1. Please try to update the array on the left, in order to get them to be equal to our_parameters. Pay attention to how sgd is implemented. . parameters_equal([p for p in m.parameters()],our_parameters) . False False False False . Remember, running Fastai&#39;s optimizer will update the weights. So you will have to rerun the above to get the above problem. . opt.step() . Here we do a comparison to Fastai&#39;s implmentation, just to make sure we are getting the same values. We use all_close here, because later on there will start to be slight variations as more math (and therefore error ! ) is introduced. . def parameters_equal(mps,ops): for mp,op in zip((mps),ops): print(mp.allclose(op)) parameters_equal(m.parameters(),our_parameters) . True True True True . SGD with Momentum . x,labels = dls.one_batch() x,labels=x.cpu(),labels.cpu() m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3200,2)) l = nn.CrossEntropyLoss() lr=0.1 mom=0.9 opt=SGD(m.parameters(),lr,mom) . pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() . class OurSGDwithMomentum: def __init__(self,params,lr,mom): self.params,self.lr=list(params),lr self.mom=mom ##added self.avg_grad=[torch.zeros_like(p) for p in self.params] #notice how avg_Grad starts at 0. def step(self): updated_params=[] for i,p in enumerate(self.params): updated_params.append(p.add(-self.lr*self.mom_grad(i,p.grad))) return updated_params #avg_grad is weighted average using momentum def mom_grad(self,i,grad): self.avg_grad[i]=self.mom*self.avg_grad[i]+grad #this is the important part return self.avg_grad[i] . Above we add momentum, it is important to realize that momentum is a weighted average and not the &quot;mean.&quot; This weighted average is used a bit in machine learning, so it is good to get this concept down now. Momentum is one of the more important hyper parameters after learning rate and weight decay(coming up next). . our_sgd=OurSGDwithMomentum(m.parameters(),lr,mom) . our_parameters=our_sgd.step() . Q2, make this one work as in Q1. Pay attention to how momentum works in the code above. . parameters_equal([p-lr*(mom+p.grad) for p in m.parameters()],our_parameters) . False False False False . opt.step() . parameters_equal(m.parameters(),our_parameters) . True True True True . Now for step #2! We do a second step, as there is &quot;state&quot; within momentum, and we need to make sure that this state carries over to the second step. . Q3. Because of state changes, update the code here to get the correct answer below. . our_answer=[p-lr*(our_sgd.avg_grad[i]+p.grad) for i,p in enumerate(m.parameters())] #loops through avg_grad now . our_parameters=our_sgd.step() . parameters_equal(our_answer,our_parameters) #this is testing your solution. . False False False False . opt.step() . parameters_equal(m.parameters(),our_parameters) . True True True True . SGD with Weight Decay . For SGD Weight Decay and l2_Regularization are effectively the same. One on the weights, one on the gradients. This is not the same for more complicated optimizers. . x,labels = dls.one_batch() x,labels=x.cpu(),labels.cpu() m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3200,2)) l = nn.CrossEntropyLoss() lr=0.1 mom=0.9 wd=0.01 opt=SGD(m.parameters(),lr,mom,wd) . pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() . class Momentum: def __init__(self,params,lr,mom): self.mom=mom self.params=params self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum def __call__(self,**kwargs): self.avg_grads = [ self.mom*avg_grad+p.grad for p,avg_grad in zip(self.params,self.avg_grads) ] return {&#39;avg_grads&#39;: self.avg_grads,**kwargs} class Weight_Decay: def __init__(self,params,lr,wd): self.lr=lr self.wd=wd self.params=params def __call__(self,**kwargs): return {**kwargs,&#39;params&#39;:[p*(1-self.lr*self.wd) for p in self.params]} #same as params-lr*wd*params, important part!!!! class OurSGD: def __init__(self,params,lr,mom,wd): self.params,self.lr=list(params),lr self.mom=Momentum(self.params,self.lr,mom) self.wd=Weight_Decay(self.params,self.lr,wd) def step(self): updated_params=[] self.params=self.wd()[&#39;params&#39;] avg_grads=self.mom()[&#39;avg_grads&#39;] for i,p in enumerate(self.params): updated_params.append(p.add(-self.lr*avg_grads[i])) return updated_params . Okay, things have gotten more complicated. We now split Momentum and Weight Decay out into two seperate functions. These are optimizer callbacks in fastai. [p (1-self.lr self.wd) for p in self.params] is the important bit, as well as understanding the order the math is applied. . our_sgd=OurSGD(m.parameters(),lr,mom,wd) . def momentum(mom,avg_grad,p): return mom*avg_grad+p.grad def weight_decay(wd): return wd our_answers=[p-lr*momentum(mom,our_sgd.mom.avg_grads[i],p)-lr*weight_decay(wd) for i,p in enumerate(m.parameters())] . our_parameters=our_sgd.step() . Q4, make this one true by editing the weight_decay function above. . parameters_equal(our_answers,our_parameters) . False False False False . opt.step() . parameters_equal(m.parameters(),our_parameters) . True True True True . Step two... . opt.zero_grad() pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() our_parameters=our_sgd.step() opt.step() parameters_equal(m.parameters(),our_parameters) . True True True True . SGD with l2 reg . l2 reg and weight decay have very similar effects, so no reason to use both. . x = torch.randn([10,3,32,32]) m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3872,2)) l = nn.CrossEntropyLoss() lr=0.1 mom=0.9 wd=0.01 opt=SGD(m.parameters(),lr,mom,wd,decouple_wd= False) . pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() . We are doing a bit of refactoring here to remove momentum and weight decay specific logic here. WE also split off the sgd_specific step while we are at it. . class Momentum: def __init__(self,params=None,lr=0.0001,mom=0.9,**kwargs): self.mom=mom self.params=params self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum def __call__(self,params=None,**kwargs): params = self.params if params is None else params self.avg_grads = [ self.mom*avg_grad+p.grad for p,avg_grad in zip(params,self.avg_grads) ] return {**kwargs,&#39;params&#39;:params,&#39;avg_grads&#39;: self.avg_grads} class Weight_Decay: def __init__(self,params=None,lr=0.0001,wd=0.01,decouple=True,**kwargs): self.lr=lr self.wd=wd self.params=params self.decouple=decouple def __call__(self,**kwargs): params = self._do_wd() if self.decouple else self._do_l2_reg() return {**kwargs,&#39;params&#39;:params} def _do_wd(self,**kwargs): params=[p*(1-self.lr*self.wd) for p in self.params] for p,mp in zip(params,self.params): p.grad=mp.grad return params #same as params-lr*wd*params #this one is pretty ugly def _do_l2_reg(self,**kwargs): params=[deepcopy(p) for p in self.params] for p,mp in zip(params,self.params): p.grad=mp.grad + self.wd* mp return params class OurSGD: hypers=[Weight_Decay,Momentum] def __init__(self,params,lr,**kwargs): self.lr=lr self.params=params def __call__(self,params=None,avg_grads=None,**kwargs): return {**kwargs,&#39;params&#39;:[ p.add(-self.lr*avg) for p,avg in zip(params,avg_grads) ]} class OurOptimizer: def __init__(self,params,lr,opt,**kwargs): self.state={&#39;params&#39;:list(params),&#39;lr&#39;:lr} self.cbs=[cls(**self.state,**kwargs) for cls in [*opt.hypers,opt]] def step(self): state=self.state for cb in self.cbs: state=cb(**state) return state[&#39;params&#39;] . our_opt=OurOptimizer(m.parameters(),lr,OurSGD,decouple=False) . our_parameters=our_opt.step() . opt.step() . parameters_equal(m.parameters(),our_parameters) . True True True True . opt.zero_grad() pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() our_parameters=our_opt.step() opt.step() parameters_equal(m.parameters(),our_parameters) . True True True True . Done with my refactoring. If you notice there is a issue of lots of for...loops going over the same data. In fastai each function momentum/weight_decay/sgd works on a single parameter at a time, and that is encapsulated in a single for...loop, instead of my approach of passing all the parameters to function that does the looping itself. I just got tired of refactoring at this point and decided to keep what I had.... lots of refactoring happened not in this notebook. . RMSProp . x = torch.randn([10,3,32,32]) m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3872,2)) l = nn.CrossEntropyLoss() lr=0.1 mom=0.9 wd=0.01 sqr_mom=0.95 opt=RMSProp(m.parameters(),lr,sqr_mom,mom,wd) . pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() . class Momentum: def __init__(self,params=None,lr=0.0001,mom=0.9,**kwargs): self.mom=mom self.params=params self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum def __call__(self,params=None,**kwargs): params = self.params if params is None else params self.avg_grads = [ self.mom*avg_grad+p.grad for p,avg_grad in zip(params,self.avg_grads) ] return {**kwargs,&#39;params&#39;:params,&#39;avg_grads&#39;: self.avg_grads} class Weight_Decay: def __init__(self,params=None,lr=0.0001,wd=0.01,decouple=True,**kwargs): self.lr=lr self.wd=wd self.params=params self.decouple=decouple def __call__(self,**kwargs): params = self._do_wd() if self.decouple else self._do_l2_reg() return {**kwargs,&#39;params&#39;:params} def _do_wd(self,**kwargs): params=[p*(1-self.lr*self.wd) for p in self.params] for p,mp in zip(params,self.params): p.grad=mp.grad return params #same as params-lr*wd*params #this one is pretty ugly def _do_l2_reg(self,**kwargs): params=[deepcopy(p) for p in self.params] for p,mp in zip(params,self.params): p.grad=mp.grad + self.wd* mp return params class OurSGD: hypers=[Weight_Decay,Momentum] def __init__(self,params,lr,**kwargs): self.lr=lr self.params=params def __call__(self,params=None,avg_grads=None,**kwargs): return {**kwargs,&#39;params&#39;:[ p.add(-self.lr*avg) for p,avg in zip(params,avg_grads) ]} class OurOptimizer: def __init__(self,params,lr,opt,**kwargs): self.state={&#39;params&#39;:list(params),&#39;lr&#39;:lr} self.cbs=[cls(**self.state,**kwargs) for cls in [*opt.hypers,opt]] def step(self): state=self.state for cb in self.cbs: state=cb(**state) return state[&#39;params&#39;] . class Learning_Rate_Decay: def __init__(self, params=None,sqr_mom=0.99,**kwargs): self.sqr_mom=sqr_mom self.sqr_avgs=[torch.zeros_like(p) for p in params] def __call__(self, params=None, dampening=True, **kwargs): damp = 1-sqr_mom if dampening else 1. self.sqr_avgs = [sqr_avg * self.sqr_mom + damp * p.grad.data ** 2 for p,sqr_avg in zip(params,self.sqr_avgs)] return { **kwargs,&#39;params&#39;:params,&#39;sqr_avgs&#39;:self.sqr_avgs} . class OurRMSProp: hypers=[Weight_Decay,Momentum,Learning_Rate_Decay] def __init__(self,lr,params,**kwargs): self.lr=lr self.params=params def __call__(self,params=None,avg_grads=None,eps=1e-08,sqr_avgs=None,**kwargs): return {**kwargs,&#39;params&#39;:[ p.add(-self.lr*avg/(sqr_avg**(0.5)+eps)) for p,avg,sqr_avg in zip(params,avg_grads,sqr_avgs) ]} . our_opt=OurOptimizer(m.parameters(),lr,OurRMSProp,sqr_mom=0.95) . our_parameters=our_opt.step() . opt.step() . parameters_equal(m.parameters(),our_parameters) . True True True True . opt.zero_grad() pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() our_parameters=our_opt.step() opt.step() parameters_equal(m.parameters(),our_parameters) . True True True True . Adam . x = torch.randn([10,3,32,32]) m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3872,2)) l = nn.CrossEntropyLoss() lr=0.1 mom=0.9 wd=0.01 eps=1e-05 sqr_mom=0.95 opt=Adam(m.parameters(),lr,mom,sqr_mom,eps,wd) . pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() . class Weight_Decay: def __init__(self,params=None,lr=0.0001,wd=0.01,decouple=True,**kwargs): self.lr=lr self.wd=wd self.params=params self.decouple=decouple def __call__(self,**kwargs): params = self._do_wd() if self.decouple else self._do_l2_reg() return {**kwargs,&#39;params&#39;:params} def _do_wd(self,**kwargs): params=[p*(1-self.lr*self.wd) for p in self.params] for p,mp in zip(params,self.params): p.grad=mp.grad return params #same as params-lr*wd*params #this one is pretty ugly def _do_l2_reg(self,**kwargs): params=[deepcopy(p) for p in self.params] for p,mp in zip(params,self.params): p.grad=mp.grad + self.wd* mp return params class OurSGD: hypers=[Weight_Decay,Momentum] def __init__(self,params,lr,**kwargs): self.lr=lr self.params=params def __call__(self,params=None,avg_grads=None,**kwargs): return {**kwargs,&#39;params&#39;:[ p.add(-self.lr*avg) for p,avg in zip(params,avg_grads) ]} class OurOptimizer: def __init__(self,params,lr,opt,**kwargs): self.state={&#39;params&#39;:list(params),&#39;lr&#39;:lr} self.cbs=[cls(**self.state,**kwargs) for cls in [*opt.hypers,opt]] def step(self): state=self.state for cb in self.cbs: state=cb(**state) return state[&#39;params&#39;] class Learning_Rate_Decay: def __init__(self, params=None,sqr_mom=0.99,**kwargs): self.sqr_mom=sqr_mom self.sqr_avgs=[torch.zeros_like(p) for p in params] def __call__(self, params=None, dampening=True, **kwargs): damp = 1-sqr_mom if dampening else 1. self.sqr_avgs = [sqr_avg * self.sqr_mom + damp * p.grad.data ** 2 for p,sqr_avg in zip(params,self.sqr_avgs)] return { **kwargs,&#39;params&#39;:params,&#39;sqr_avgs&#39;:self.sqr_avgs} class OurRMSProp: hypers=[Weight_Decay,Momentum,Learning_Rate_Decay] def __init__(self,lr,params,**kwargs): self.lr=lr self.params=params def __call__(self,params=None,avg_grads=None,eps=1e-08,sqr_avgs=None,**kwargs): return {**kwargs,&#39;params&#39;:[ p.add(-self.lr*avg/(sqr_avg**(0.5)+eps)) for p,avg,sqr_avg in zip(params,avg_grads,sqr_avgs) ]} . class Step: def __init__(self,**kwargs): self.step=0 def __call__(self,**kwargs): self.step+=1 return {&#39;step&#39;:self.step,**kwargs} class Momentum: def __init__(self,params=None,lr=0.0001,mom=0.9,**kwargs): self.mom=mom self.params=params self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum def __call__(self,params=None,**kwargs): params = self.params if params is None else params self.avg_grads = [ self.mom*avg_grad+(1-self.mom)*p.grad for p,avg_grad in zip(params,self.avg_grads) ] return {**kwargs,&#39;params&#39;:params,&#39;avg_grads&#39;: self.avg_grads} class OurAdam: hypers=[Weight_Decay,Momentum,Learning_Rate_Decay,Step] def __init__(self,lr,params,mom=0.9,sqr_mom=0.99,eps=1e-08,**kwargs): self.lr=lr self.params=params self.mom=mom self.sqr_mom=sqr_mom self.eps=eps def __call__(self,step=1,params=None,avg_grads=None,sqr_avgs=None,**kwargs): #eps=1e-08 sqr_avgs=[sqr_avg/(1 - sqr_mom**step) for sqr_avg in sqr_avgs] avg_grads = [avg_grad / (1 - mom**step) for avg_grad in avg_grads] return {**kwargs,&#39;params&#39;:[ p.addcdiv( -lr ,grad_avg,(sqr_avg.sqrt() + self.eps )) for p,grad_avg,sqr_avg in zip(params,avg_grads,sqr_avgs) ]} . our_opt=OurOptimizer(m.parameters(),lr,OurAdam,eps=eps,sqr_mom=0.95) . our_parameters=our_opt.step() . opt.step() . #shows parameters close to not being equal def parameters_equal_show(mps,ops): for mp,op in zip((mps),ops): print(mp.masked_select((mp-op).abs()&gt;1e-08),op.masked_select((mp-op).abs()&gt;1e-08)) break . parameters_equal_show(m.parameters(),our_parameters) . tensor([ 0.1133, 0.1766, 0.2099, 0.1881, 0.1587, 0.2367, 0.2189, 0.2101, 0.1876, -0.2224, -0.2393, -0.1429, 0.2033, 0.1898, -0.1359, -0.1989, -0.2205, 0.1676, -0.2199, -0.1297, -0.1014, -0.1833, -0.1241, -0.1455, -0.2361, 0.0979, -0.1395, -0.1275, 0.1032, 0.1991, -0.1265, -0.2069, -0.2465, 0.2102, -0.0968, -0.2100, 0.1213, -0.1896, 0.1769, -0.2089, -0.1518, 0.1592, 0.2491, 0.2233, -0.2382, -0.1474, 0.1746, -0.2236, 0.2342, -0.1417, 0.1879, -0.1650, -0.2419, -0.1232, -0.2413, -0.2258, 0.1095, -0.1980, -0.1882, -0.1661, -0.1769, 0.2334, -0.1878, 0.1639, -0.2143, 0.1531, -0.1679, 0.2016, -0.1644, 0.1493, 0.2032, -0.1417, -0.1873, 0.1333, -0.1954, 0.1659, -0.1852, -0.1532, -0.1191, -0.2032, 0.2201, -0.1217, 0.1954, -0.1297, 0.2288, -0.1301, -0.2084, -0.2102, -0.1571, -0.1803, -0.1516, 0.2013, -0.2338, -0.2348, 0.2308, -0.1777, -0.1932, 0.2142, -0.1990, 0.1751, -0.1063, 0.1684, 0.1321, -0.2281, 0.2266, 0.2006, -0.1572, -0.1618, 0.1587, -0.1836, -0.1399, 0.1631, 0.2076, -0.1600, 0.1584, 0.1980, -0.1637, -0.2027, 0.1153, -0.1639, 0.2354, 0.2395, 0.1963, 0.1393, 0.1422, -0.2111, 0.1376, -0.2402, -0.1623, 0.2126, -0.2116, 0.1634, 0.2348, -0.1085, 0.1771, 0.1734, 0.2279, -0.0948, 0.2020, 0.1733, 0.1934, -0.2217, -0.1422, -0.2375, 0.1274, 0.0943, 0.1806, -0.2292, -0.2484, 0.0879, -0.0870, 0.2374, -0.2179, 0.1361, -0.2142, 0.1657, 0.2259, 0.1768, -0.1184, 0.1224, 0.1537, 0.2078, -0.1869, 0.1412, -0.1900, 0.2006, 0.1538, -0.2469, 0.1525, 0.1741, -0.1754, -0.1602, 0.1738, -0.1929, -0.2245, -0.1532, 0.2110, -0.1917, -0.2124, -0.1408, 0.2142, 0.1549, 0.2338, 0.1441, 0.1377, -0.0856, 0.1355, 0.1945, 0.1407, -0.1251, -0.1614, -0.1672, -0.1015, -0.2143, -0.2052, -0.1670, -0.1607, -0.1440, 0.1313, -0.2089, -0.1839, 0.1929, -0.1760, -0.1672, 0.1439, 0.2235, 0.1692, -0.2194, 0.2251, 0.2327, 0.1336, 0.2163, 0.1336, 0.1259, -0.1727, 0.2150, 0.1043, -0.2463, -0.1123, -0.2426, -0.1792, -0.2002, 0.1751, 0.2148, 0.1448, -0.1380, -0.1563, -0.1871, 0.1831, -0.2145, 0.1607, -0.1265, -0.1911, 0.1194, -0.2165, 0.1499, -0.2241, 0.2112, -0.2148, 0.2091, -0.1458, -0.1751, -0.1461, -0.0967, -0.2017, 0.2188, -0.2264, -0.1651, 0.1476, 0.1527, 0.1455, 0.1243, -0.1918, 0.2038, 0.1374, -0.1014, 0.1878, 0.1480, 0.2019, -0.2465, 0.1896, -0.0874, 0.1408, 0.1890, -0.1063, -0.1180, 0.2152, -0.2209, -0.0888, -0.1289, -0.0962, -0.1288, -0.1128, -0.2351, -0.1592, 0.2114, -0.1509, 0.1655, 0.1508, -0.2424, 0.1167, -0.1288, 0.1192, 0.2172, 0.2097, 0.1360, 0.1987, 0.1679, -0.2080, 0.1977, 0.1119, -0.2058, 0.2321, -0.2175, 0.1355, -0.1258, 0.2299, -0.1452, 0.1001, 0.2152, 0.1421, 0.1147, 0.2330, -0.1812, -0.2164, -0.1955, -0.2077, 0.0908, -0.1649, 0.1881, -0.1824, 0.2457, -0.2432, 0.1121, -0.1637, -0.2450, 0.1257, -0.1627, 0.1748, 0.2427, -0.1599, -0.1931, -0.1066, 0.1651, -0.1312, -0.1043, -0.2015, -0.1110, 0.1249, 0.1858, 0.2090, 0.1888, -0.1954, -0.2169, 0.2114, 0.1973, -0.2127, -0.2306, 0.2417, -0.2018, 0.1643, -0.1919, 0.1861, -0.2181, 0.1240, 0.1651, -0.1849, 0.1898, 0.2095, -0.1603, 0.2033, 0.1576, 0.2191, -0.2073, 0.1760, -0.2221, 0.2012, -0.2427, -0.2261, -0.2418, 0.0866, -0.1986, 0.1042, 0.1729, -0.2225, 0.2179, -0.2128, -0.2064, 0.0927, -0.1463, -0.1697, -0.1840, -0.1575, -0.1765, -0.1417, -0.2115, 0.2077, 0.1541, -0.1145, 0.2130, -0.2225, -0.1029, -0.1301, 0.2200, -0.1071, -0.2340, 0.1184, -0.1121, 0.2498, -0.1454, -0.1599, -0.1192, 0.2128, -0.2047, -0.1026, -0.1550, 0.1381, 0.1884, -0.1259, 0.1918, -0.1030, -0.1590, -0.2230, 0.2229, -0.2175, 0.1678, -0.2143, -0.1747, -0.1552, 0.1536, 0.1499, 0.1595, -0.1967, -0.1533, -0.2196, 0.1193, -0.1477, 0.2348, 0.2465, -0.2066, 0.1739, -0.1321, 0.2126, 0.1777, -0.1872, 0.1361, -0.2462, 0.1694, 0.1474, 0.1672, 0.1734, -0.1947, 0.1702, -0.1690, 0.1698, -0.1295, 0.1401, -0.1864, -0.1566, 0.1362, 0.1980, 0.1747, -0.2286, -0.1648, 0.2033, -0.1179, 0.1884, 0.1921, -0.2183, 0.2279, -0.2139, -0.0851, 0.2477, 0.1419, -0.1451, -0.1677, 0.2460, 0.2305, 0.1868, -0.2343, -0.1505, 0.2287, 0.1314, 0.2475, -0.1446, -0.1734, 0.1849, 0.1168, -0.2334, -0.2158, -0.2291, -0.2386, 0.1709, -0.2310, -0.1924, -0.2474, -0.1943, -0.2114, -0.2292, 0.1596, -0.2397, 0.2005, 0.1538, -0.2183, -0.2213, 0.1287, 0.1810, 0.1724, 0.1620, -0.1071, 0.1933, 0.2109, -0.2471, -0.1865, 0.2191, 0.1049, 0.1630, 0.1005, -0.1664, 0.1153, 0.1687, -0.0914, -0.2499, 0.1455, -0.1582, -0.2422, -0.1850, 0.1949, 0.2278, 0.1610, -0.1282, 0.2283, 0.2352, 0.1533, -0.2019, -0.1028, 0.1885, 0.1379, 0.1058, 0.1140, -0.2087, -0.1397, -0.1373, 0.2404, 0.2000, -0.1603, 0.1448, -0.2044, 0.1402, 0.2451, 0.1484, 0.1715, 0.2371, -0.1006, 0.1071, -0.1647, 0.1361, -0.1388, -0.1953, -0.2061, 0.1237, -0.2190, -0.2424, 0.2041, 0.2076, -0.2180, -0.1903, -0.1864, 0.1390, 0.2429, 0.1950, 0.2327, -0.1784, -0.2185, 0.1452, 0.1724, 0.0899, -0.0891, -0.1403, 0.1413, 0.2000, 0.1347, -0.2395, -0.1211, 0.1891, -0.1858, 0.2475, 0.2388, -0.1659, -0.2133, 0.2159, 0.2232, 0.1672, 0.1641, -0.2145, 0.1735, -0.1986, 0.1812, 0.1669, -0.1667, -0.1812, -0.1491, -0.1656, 0.2409, -0.1922, 0.1408, -0.1686, 0.2273, -0.1791, -0.1835, -0.1541, -0.1754, 0.2418, 0.2302, 0.2209, -0.1581, -0.1870, 0.1369, 0.2049, 0.2351, 0.2441, 0.2287, 0.2472, -0.1304, 0.2357, 0.1048, -0.2482, 0.1144, 0.1021, -0.1148, 0.1297, 0.1271, 0.1215, 0.1378], grad_fn=&lt;MaskedSelectBackward&gt;) tensor([ 0.1133, 0.1766, 0.2099, 0.1881, 0.1587, 0.2367, 0.2189, 0.2101, 0.1876, -0.2224, -0.2393, -0.1429, 0.2033, 0.1898, -0.1359, -0.1989, -0.2205, 0.1676, -0.2199, -0.1297, -0.1014, -0.1833, -0.1241, -0.1455, -0.2361, 0.0979, -0.1395, -0.1275, 0.1032, 0.1991, -0.1265, -0.2069, -0.2465, 0.2102, -0.0968, -0.2100, 0.1213, -0.1896, 0.1769, -0.2089, -0.1518, 0.1592, 0.2491, 0.2233, -0.2382, -0.1474, 0.1746, -0.2236, 0.2342, -0.1417, 0.1879, -0.1650, -0.2419, -0.1232, -0.2413, -0.2258, 0.1095, -0.1980, -0.1882, -0.1661, -0.1769, 0.2334, -0.1878, 0.1639, -0.2143, 0.1531, -0.1679, 0.2016, -0.1644, 0.1493, 0.2032, -0.1417, -0.1873, 0.1333, -0.1954, 0.1659, -0.1852, -0.1532, -0.1191, -0.2032, 0.2201, -0.1217, 0.1954, -0.1297, 0.2288, -0.1301, -0.2084, -0.2102, -0.1571, -0.1803, -0.1516, 0.2013, -0.2338, -0.2348, 0.2308, -0.1777, -0.1932, 0.2142, -0.1990, 0.1751, -0.1063, 0.1684, 0.1321, -0.2281, 0.2266, 0.2006, -0.1572, -0.1618, 0.1587, -0.1836, -0.1399, 0.1631, 0.2076, -0.1600, 0.1584, 0.1980, -0.1637, -0.2027, 0.1153, -0.1639, 0.2354, 0.2395, 0.1963, 0.1393, 0.1422, -0.2111, 0.1376, -0.2402, -0.1623, 0.2126, -0.2116, 0.1634, 0.2348, -0.1085, 0.1771, 0.1734, 0.2279, -0.0948, 0.2020, 0.1733, 0.1934, -0.2217, -0.1422, -0.2375, 0.1274, 0.0943, 0.1806, -0.2292, -0.2484, 0.0879, -0.0870, 0.2374, -0.2179, 0.1361, -0.2142, 0.1657, 0.2259, 0.1768, -0.1184, 0.1224, 0.1537, 0.2078, -0.1869, 0.1412, -0.1900, 0.2006, 0.1538, -0.2469, 0.1525, 0.1741, -0.1754, -0.1602, 0.1738, -0.1929, -0.2245, -0.1532, 0.2110, -0.1917, -0.2124, -0.1408, 0.2142, 0.1549, 0.2338, 0.1441, 0.1377, -0.0856, 0.1355, 0.1945, 0.1407, -0.1251, -0.1614, -0.1672, -0.1015, -0.2143, -0.2052, -0.1670, -0.1607, -0.1440, 0.1313, -0.2089, -0.1839, 0.1929, -0.1760, -0.1672, 0.1439, 0.2235, 0.1692, -0.2194, 0.2251, 0.2327, 0.1336, 0.2163, 0.1336, 0.1259, -0.1727, 0.2150, 0.1043, -0.2463, -0.1123, -0.2426, -0.1792, -0.2002, 0.1751, 0.2148, 0.1448, -0.1380, -0.1563, -0.1871, 0.1831, -0.2145, 0.1607, -0.1265, -0.1911, 0.1194, -0.2165, 0.1499, -0.2241, 0.2112, -0.2148, 0.2091, -0.1458, -0.1751, -0.1461, -0.0967, -0.2017, 0.2188, -0.2264, -0.1651, 0.1476, 0.1527, 0.1455, 0.1243, -0.1918, 0.2038, 0.1374, -0.1014, 0.1878, 0.1480, 0.2019, -0.2465, 0.1896, -0.0874, 0.1408, 0.1890, -0.1063, -0.1180, 0.2152, -0.2209, -0.0888, -0.1289, -0.0962, -0.1288, -0.1128, -0.2351, -0.1592, 0.2114, -0.1509, 0.1655, 0.1508, -0.2424, 0.1167, -0.1288, 0.1192, 0.2172, 0.2097, 0.1360, 0.1987, 0.1679, -0.2080, 0.1977, 0.1119, -0.2058, 0.2321, -0.2175, 0.1355, -0.1258, 0.2299, -0.1452, 0.1001, 0.2152, 0.1421, 0.1147, 0.2330, -0.1812, -0.2164, -0.1955, -0.2077, 0.0908, -0.1649, 0.1881, -0.1824, 0.2457, -0.2432, 0.1121, -0.1637, -0.2450, 0.1257, -0.1627, 0.1748, 0.2427, -0.1599, -0.1931, -0.1066, 0.1651, -0.1312, -0.1043, -0.2015, -0.1110, 0.1249, 0.1858, 0.2090, 0.1888, -0.1954, -0.2169, 0.2114, 0.1973, -0.2127, -0.2306, 0.2417, -0.2018, 0.1643, -0.1919, 0.1861, -0.2181, 0.1240, 0.1651, -0.1849, 0.1898, 0.2095, -0.1603, 0.2033, 0.1576, 0.2191, -0.2073, 0.1760, -0.2221, 0.2012, -0.2427, -0.2261, -0.2418, 0.0866, -0.1986, 0.1042, 0.1729, -0.2225, 0.2179, -0.2128, -0.2064, 0.0927, -0.1463, -0.1697, -0.1840, -0.1575, -0.1765, -0.1417, -0.2115, 0.2077, 0.1541, -0.1145, 0.2130, -0.2225, -0.1029, -0.1301, 0.2200, -0.1071, -0.2340, 0.1184, -0.1121, 0.2498, -0.1454, -0.1599, -0.1192, 0.2128, -0.2047, -0.1026, -0.1550, 0.1381, 0.1884, -0.1259, 0.1918, -0.1030, -0.1590, -0.2230, 0.2229, -0.2175, 0.1678, -0.2143, -0.1747, -0.1552, 0.1536, 0.1499, 0.1595, -0.1967, -0.1533, -0.2196, 0.1193, -0.1477, 0.2348, 0.2465, -0.2066, 0.1739, -0.1321, 0.2126, 0.1777, -0.1872, 0.1361, -0.2462, 0.1694, 0.1474, 0.1672, 0.1734, -0.1947, 0.1702, -0.1690, 0.1698, -0.1295, 0.1401, -0.1864, -0.1566, 0.1362, 0.1980, 0.1747, -0.2286, -0.1648, 0.2033, -0.1179, 0.1884, 0.1921, -0.2183, 0.2279, -0.2139, -0.0851, 0.2477, 0.1419, -0.1451, -0.1677, 0.2460, 0.2305, 0.1868, -0.2343, -0.1505, 0.2287, 0.1314, 0.2475, -0.1446, -0.1734, 0.1849, 0.1168, -0.2334, -0.2158, -0.2291, -0.2386, 0.1709, -0.2310, -0.1924, -0.2474, -0.1943, -0.2114, -0.2292, 0.1596, -0.2397, 0.2005, 0.1538, -0.2183, -0.2213, 0.1287, 0.1810, 0.1724, 0.1620, -0.1071, 0.1933, 0.2109, -0.2471, -0.1865, 0.2191, 0.1049, 0.1630, 0.1005, -0.1664, 0.1153, 0.1687, -0.0914, -0.2499, 0.1455, -0.1582, -0.2422, -0.1850, 0.1949, 0.2278, 0.1610, -0.1282, 0.2283, 0.2352, 0.1533, -0.2019, -0.1028, 0.1885, 0.1379, 0.1058, 0.1140, -0.2087, -0.1397, -0.1373, 0.2404, 0.2000, -0.1603, 0.1448, -0.2044, 0.1402, 0.2451, 0.1484, 0.1715, 0.2371, -0.1006, 0.1071, -0.1647, 0.1361, -0.1388, -0.1953, -0.2061, 0.1237, -0.2190, -0.2424, 0.2041, 0.2076, -0.2180, -0.1903, -0.1864, 0.1390, 0.2429, 0.1950, 0.2327, -0.1784, -0.2185, 0.1452, 0.1724, 0.0899, -0.0891, -0.1403, 0.1413, 0.2000, 0.1347, -0.2395, -0.1211, 0.1891, -0.1858, 0.2475, 0.2388, -0.1659, -0.2133, 0.2159, 0.2232, 0.1672, 0.1641, -0.2145, 0.1735, -0.1986, 0.1812, 0.1669, -0.1667, -0.1812, -0.1491, -0.1656, 0.2409, -0.1922, 0.1408, -0.1686, 0.2273, -0.1791, -0.1835, -0.1541, -0.1754, 0.2418, 0.2302, 0.2209, -0.1581, -0.1870, 0.1369, 0.2049, 0.2351, 0.2441, 0.2287, 0.2472, -0.1304, 0.2357, 0.1048, -0.2482, 0.1144, 0.1021, -0.1148, 0.1297, 0.1271, 0.1215, 0.1378], grad_fn=&lt;MaskedSelectBackward&gt;) . parameters_equal(m.parameters(),our_parameters) . True True True True . opt.zero_grad() pred=m(x) loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long)) loss.backward() our_parameters=our_opt.step() opt.step() parameters_equal(m.parameters(),our_parameters) . True True True True . opt.hypers . (#1) [{&#39;wd&#39;: 0.01, &#39;sqr_mom&#39;: 0.95, &#39;lr&#39;: 0.1, &#39;mom&#39;: 0.9, &#39;eps&#39;: 1e-05}] . export2html.notebook2html(fname=&#39;2020-07-15-Optimizers.ipynb&#39;, dest=&#39;html/&#39;, template_file=&#39;fastpages.tpl&#39;,n_workers=1) . converting: 2020-07-15-Optimizers.ipynb . /home/fast/anaconda3/envs/fastai2/lib/python3.6/site-packages/jupyter_client/manager.py:358: FutureWarning: Method cleanup(connection_file=True) is deprecated, use cleanup_resources(restart=False). FutureWarning) .",
            "url": "https://marii-moe.github.io/marii-blog/2020/07/15/Optimizers.html",
            "relUrl": "/2020/07/15/Optimizers.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Training UGATIT in FP16",
            "content": "from nbdev import export2html from pathlib import Path from PIL import Image . Training U-GAT-IT with fp16 . Hello, this is one of my full-time self learning projects. This was done in fastai2⁹, my GPU setups for this will be two 2070 Supers. The goal of this was to use fp16 training on U-GAT-IT. I did not do any of this in a controlled and meticulous experiment fashion. While I do agree that this is very important, I found that I had to forgo this in order to get something working. For the first two months of this I did not even have a working model for example. The target audience for this post is someone with either a single MOOC or introductory deep learning class, if you are in this category and do not understand something, feel free to leave a comment and I will try to answer your question, and update the article to be clearer. If you do not have any knowledge on deep learning this article might be a bit difficult, feel free to ask questions and I will do the best to answer them, and I really recommend learning fastai at https://course.fast.ai/ . Results . Outputs . Image.open(&#39;imgs/big_img13.png&#39;) . Inputs . Image.open(&#39;imgs/big_img11.png&#39;) . Background . GANs³ are generational adversarial networks, or in this case, a network than generates images, and another network that tells if it is real or fake. The two networks then compete to be better than the other, and that is how we get a network that can generate images. CycleGan² in based off of this idea, but the generators take an image as input, the output of a generator is sent through the second generator, and and the output of the second generator must match the input of the first generator. This way a generator can&#39;t learn to simply generate an image regardless of its input, and must encode information from the original image all throughout the whole &quot;Cycle&quot;. Network architecture for The Generator of U-GAT-IT¹Network architecture for The Discriminator of U-GAT-IT¹This post is based on the U-GAT-IT¹ architecture above, and is a project I worked on in order to get U-GAT-IT training on my GPUs in fp16. U-GAT-IT showed good results on anime based images. It introduces a few things, such as the selfie2anime dataset, adaptive layer-instance norm, Class-Activation Map Loss, and more. Those who have seen the 2019 fastai course should know that Class Activation Mapping is how you get a heatmap of your network activating on a particular image. The architecture of the network is based on Cycle GAN, and has a total of two generators and four discriminators. . FP16 . Getting fp16 training working for U-GAT-IT was definitely what I spent the majority of the past few months doing. To get this to work I needed to use Mixed Precision Training⁴. I tried many things, but eventually found that the &quot;most&quot; code, with the &quot;most&quot; complicated looking training loop was the way to go, definitely something I am not used to saying. This meant that many of my earlier simpler but unsuccessful experiments meant that I was very disheartened by the time I actually got anything working. This &quot;complicated&quot; way was implementing loss scaling separately for both the generators and discriminator of U-GAT-IT. This came with challenges about how to handle overflow and tracking all the variables associated with loss scaling separately. . Reduced range in fp16⁴Now I will start with a few definitions, fp16 overflow is when your gradients are too big too fit within fp16. Underflow is when fp16 gradients are so close to 0 that they are simply rounded to 0. Overflow and underflow can happen in fp32 as well, but isn&#39;t nearly as big of a problem because fp32 is simply &quot;bigger.&quot; Loss scaling is multiplying our loss by as large a value as possible,which effectively is multiplied by our gradients. This avoids underflow by increasing the gradients, but we need to make sure it is not big enough that it causes an overflow. One technicality to loss scaling is calculating the loss itself in fp32, because calculating a loss includes division and low precision division is very in-precise leading to unstable gradients/training. Another technicality is that the gradients have to be divided by the loss scale, to keep the magnitudes consistent with a fp32-bit model, otherwise this would be very similar to simply increasing the learning rate by the loss scale! But, wait a minute… if we divide by the loss scale won&#39;t we just run into the underflow issue from before? Your right, we have to have a fp32 copy of the weights as well, so that we can avoid this problem. We copy the scaled gradients over to an fp32 model and then divide by the loss scale there. We then continue the optimization step in fp32. . So, I haven&#39;t talked about how I handled overflow yet, because that actually gets into the GAN specifics of the training loop. I found that the discriminator was unable to converge without a fairly high loss scale, and the generator overflowed at a comparatively low loss scale. This was because the generator has a much higher loss, but I will get into that latter, or you can skip to &quot;Tanh Loss&quot;. This differing requirement of loss scale means that we need to track the loss scale separately for the generator and discriminator. We also have two states for overflow failure, overflow in the generator, and overflow in the discriminator. In the case of the discriminator I simply grab another set of samples from the training set, and generate a new fake and compare to real as usual. For a generator overflow, I simply skip to the next batch. Skipping to the next batch is a problem because the total number of successfully run batches is not accounted for currently in my training loop, so there is some variations in the amount of training done, I would like to fix this in the future. . Batch Size of 3 . Batch size of three was the maximum I could reasonably get out of my model, given memory limitations. In a GAN architecture there are trade offs of increasing the batch size. I found that in my case, the very first convolution layer had a tendency to overflow, at fairly irregular intervals. Increasing the batch size to three mostly smoothed this out so that I could have orders of magnitude higher loss scale. . 2-category Categorical Cross Entropy VS Binary Cross Entropy with Logits . For people not overly familiar with loss functions, Categorical Cross Entropy is the loss function generally used for selecting a single category, out of many. Examples of tasks that use Categorical Cross Entropy are Imagenet Classification or Dog vs Cat. Dog vs Cat is a special case, because there are only two choices, Dog or Cat, this is what I am referring to when I say 2-category Categorical Cross Entropy. Binary Cross Entropy with Logits is very similar, but uses a Sigmoid into a Binary Cross Entropy Loss, which is calculating loss based on distance from either 0 or 1. This means that Categorical Cross Entropy requires 2 values one for each category, and Binary cross entropy calculates loss based on one value ranging from 0 to 1. In order to avoid overflow, I wanted to be able to raise weight decay to the largest value possible, well Binary Cross Entropy works by sending values through a Sigmoid and then seeing if the value is close to 1, but weight decay pushes these values to 0, which after going through Sigmoid equals 0.5. So, instead I decided to use loss that used Softmax instead. This allows small values to be &quot;highly confident&quot;, because if you have two small values (0.0099,0.00001) you can still get large numbers to send into the loss calculation (0.0099/(0.0099+0.00001)). . Tanh Loss . loss_A += u.cam_weight * self.tanh(u.cam_loss(fake_B2A_cam_logit,fake_A2A_cam_logit)) . Before I talk about why I decided to change up the loss function, I think it is important to understand a bit about CAM loss, which was mentioned previously. CAM loss is based on class activation mapping⁸, and is used in U-GAT-IT as part of a differential loss function. This important thing here is that CAM loss is multiplied by 1000. There are two functions of CAM loss which behave similarly, so for our calculations lets just say that cam loss is 2000loss. The maximum value for fp16 is 65504 so if we divide this by our multiplier we find that we overflow at a loss of approximately 32. Considering the fact that this loss can easily be over 1, we are calculating gradients for every parameter, and we get less precise the closer we get to this maximal value, hard decisions had to be made to simply get this model training on fp16. I decided to send the CAM loss through a hyperbolic tangent, which effectively limits its maximum value to 1, before adding it to the rest of the loss function. This meant that the formula for cam loss went from loss = 2000loss to 2000*tanh(loss). Limiting the maximum value of this loss to 1 downshifts the importance of cam loss in the beginning when the model starting training, but allows it to quickly ramp up when the model starts to get it right. This seems to cause a big shift in the loss when the gradient of this loss is quickly increasing. . ,&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Normalization . It is very important to add that I added extra normalization layers, I have not done proper testing to determine the effect of these layers, but they did decrease the magnitude of the gradients which was the intended effect. All normalization had to of course be done in fp32, whether I added it or it was already part of the network. . Tooling . This work used fastai2⁹, which is much more hackable than previous versions of fastai. I started working on this last October, so at the time there was not very much support for GAN training ported into fastai2. Still, the hackable nature of fastai2 let me work my training loop into the one used for regular image classification and similar tasks. Fastai2 really provides you a great way to help organize your code, even when you are not using models or training loops in a purely out-of-the-box sort of way. The dataloading was also improved in this version, and allowed for things like on GPU data augmentations, and easy way to compose this data pipeline. I also was able to edit much of the code there to add in functionality that I was trying to create. Having the WandbCallback in fastai was instrumental in tracking training and identifying where I should spend time optimizing training. By tracking gradients in weights and biases¹⁰ as well as all of the other hyper parameters, I was able to easily identify what my current goal should be, as well as have some hope that the network would eventually train. Watching for small improvements in logs over the course of training was the only way I could tell the model was working better in fp16, as the output of the model didn&#39;t really improve much. . Contact Me . Linked-In Profile: https://www.linkedin.com/in/molly-beavers-651025118/ . Source Code: https://github.com/marii-moe/selfie2anime . References . [1] Junho Kim, Minjae Kim, Hyeonwoo Kang and Kwang Hee Lee. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation. International Conference on Learning Representations. 2020 https://openreview.net/forum?id=BJlZ5ySKPH [2] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. &quot;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&quot;, in IEEE International Conference on Computer Vision (ICCV), 2017. (* indicates equal contributions). https://arxiv.org/pdf/1703.10593.pdf [3] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio.&quot;Generative Adversarial Networks.&quot; ArXiv, 2014. https://arxiv.org/abs/1406.2661 [4] Micikevicius, P., Narang, S., Alben, J., Diamos, G.F., Elsen, E., García, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., &amp; Wu, H.&quot;Mixed Precision Training&quot;. ArXiv, 2017. https://arxiv.org/abs/1710.03740 [5] Chen, T., Xu, B., Zhang, C., &amp; Guestrin, C. &quot;Training Deep Nets with Sublinear Memory Cost&quot;. ArXiv, 2016. https://arxiv.org/abs/1604.06174 [6] Jason Antic. &quot;DeOldify&quot;. https://github.com/jantic/DeOldify [7] Hicsonmez, Samet &amp; Samet, Nermin &amp; Akbas, Emre &amp; Duygulu, Pinar. &quot;GANILLA: Generative adversarial networks for image to illustration translation&quot;. Elsevier Image and Vision Computing 103886, 2020. https://arxiv.org/abs/2002.05638 [8] Zhou, B. &amp; Khosla, A. and Lapedriza. A. and Oliva, A. and Torralba, A. &quot;Learning Deep Features for Discriminative Localization.&quot; CVPR, 2016. http://cnnlocalization.csail.mit.edu/ [9] Howard, J., &amp; Gugger, S. fastai: A Layered API for Deep Learning. ArXiv, 2020. https://arxiv.org/abs/2002.04688 [10] Biewald, Lukas. &quot;Experiment Tracking with Weights and Biases&quot;. 2020. https://www.wandb.com/ . export2html.notebook2html(fname=&#39;2020-06-11-UGATIT-a-GAN-in-fp16.ipynb&#39;, dest=&#39;html/&#39;, template_file=&#39;fastpages.tpl&#39;,n_workers=1) . converting: 2020-06-11-UGATIT-a-GAN-in-fp16.ipynb . &lt;/div&gt; .",
            "url": "https://marii-moe.github.io/marii-blog/ugatit/fp16/gan/image%20generation/image%20to%20image/2020/06/11/UGATIT-a-GAN-in-fp16.html",
            "relUrl": "/ugatit/fp16/gan/image%20generation/image%20to%20image/2020/06/11/UGATIT-a-GAN-in-fp16.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Training GANs",
            "content": "from nbdev import export2html from fastai2.vision.gan import * from fastai2.basics import * from fastai2.vision.all import * . For a better understanding of GANs, and how fastai2 does GAN training, take a look at the source notebook here: https://github.com/fastai/fastai2/blob/master/nbs/24_vision.gan.ipynb . GANS . GANs³ are generational adversarial networks, or in this case, a network than generates images, and another network,the critic, that tells if it is real or fake. The two networks then compete to be better than the other, and that is how we get a network that can generate images. . DataLoading . In this section we will setup a dataloader in fastai to handle loading in data for GANS. We will need a data loader that provides data for both the generator and the dicriminator. . bs = 128 size = 64 . db = DataBlock(blocks = (TransformBlock, ImageBlock), get_x = generate_noise, get_items = get_image_files, splitter = IndexSplitter([]), item_tfms=Resize(size, method=ResizeMethod.Crop), batch_tfms = Normalize.from_stats(torch.tensor([0.5,0.5,0.5]), torch.tensor([0.5,0.5,0.5]))) . path=Path(&#39;/home/fast/.fastai/data/selfie2anime/trainA&#39;) . dls = db.dataloaders(path, path=path, bs=bs) . dls.show_batch(max_n=16) . As seen above, we need to provide examples of real images. These are given to the critic to teach it what real images look like. We will also feed it fake images from the generator during the training loop. . Below is an InvisibleTensor, that is fed to the generator. It is invisible and not shown when we show a batch, mostly because it has no real meaning to us, and is instead just a random number. . dls.one_batch()[0][0:3,0:20] . tensor([[-0.9633, -0.9656, 1.2281, -0.9988, -0.1541, -0.2493, 0.2096, -0.2743, 0.7135, 0.0566, -0.3747, -0.8285, -1.2817, -1.4126, 0.2445, 1.1226, -1.2934, -2.3484, -0.4882, 1.7837], [-2.1919, 0.4817, -0.7841, -0.6734, 0.9772, -1.1276, -0.5570, 0.3376, 0.6676, -0.3482, -0.0642, -0.3559, 1.2127, -1.0446, -1.3443, -0.9860, -0.1398, 0.3522, 2.9911, -2.3752], [-0.2121, 1.9836, -0.3058, -0.9480, -0.7573, 0.2996, -0.9074, -0.8229, -0.9196, 2.7629, 0.5457, 0.3248, 1.1394, -0.3251, 0.3291, 0.5986, 0.1644, 1.0541, -1.1510, -0.0818]], device=&#39;cuda:0&#39;) . generator = basic_generator(64, n_channels=3, n_extra_layers=1) critic = basic_critic (64, n_channels=3, n_extra_layers=1, act_cls=partial(nn.LeakyReLU, negative_slope=0.2)) . learn = GANLearner.wgan(dls, generator, critic, opt_func = RMSProp) . We have to turn off validation metrics due to their not being a validation set for GANs, we also create a callback so the validation run stops early. We turn on training metrics so that we can track generator and critic loss seperately. . learn.recorder.train_metrics=True learn.recorder.valid_metrics=False . with learn.no_logging(): with learn.no_bar(): learn.fit(50, 2e-4, wd=0.) . learn.recorder.plot_loss() . learn.show_results(max_n=9, ds_idx=0) . As you can see, the results are not the best right off the bat. You can get better results by training for longer, and finetuning the model more, but I think it is also important to show that GANs can take awhile to train. . with learn.no_logging(): with learn.no_bar(): learn.fit(500, 2e-4, wd=0.) . learn.recorder.plot_loss() . Here is what we get after 500 epochs. With this GAN I wouldn&#39;t expect great results. GANs in general have a lot of parameters and are hard to train. U-NET based GANS do fairly well for a low parameter Generators though. U-NETs are conditional Generators that take an image as an input. Feel free to look at other architectures as well, though know that they can take a long time to train. . learn.show_results(max_n=9, ds_idx=0) . And here is the generator model architecture. This is just like a CNN, except that it uses ConvTranspose2D . learn.model.generator . Sequential( (0): AddChannels() (1): ConvLayer( (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): ConvLayer( (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (4): ConvLayer( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (5): ConvLayer( (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (6): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (7): Tanh() ) . rand is going to be our input into our ConvTranspose layer. It is just random noise. . rand=torch.randn([1,100],device=default_device()) . Here we are using an identity matrix * 5, we are going to add this to blow up specific weights. This is just to show example of how the random input gets mapped to different features. . ones = 5*torch.eye(100,device=default_device()) ones[0:5,0:5] . tensor([[5., 0., 0., 0., 0.], [0., 5., 0., 0., 0.], [0., 0., 5., 0., 0.], [0., 0., 0., 5., 0.], [0., 0., 0., 0., 5.]], device=&#39;cuda:0&#39;) . alter_single_params=(rand+ones)[0:10] rand[:,0:10],alter_single_params[:,0:10] . (tensor([[ 0.9892, -0.3591, -0.2757, -0.1560, 0.6927, 1.3321, -1.4985, 0.6510, 0.1489, 1.4966]], device=&#39;cuda:0&#39;), tensor([[ 5.9892, -0.3591, -0.2757, -0.1560, 0.6927, 1.3321, -1.4985, 0.6510, 0.1489, 1.4966], [ 0.9892, 4.6409, -0.2757, -0.1560, 0.6927, 1.3321, -1.4985, 0.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, 4.7243, -0.1560, 0.6927, 1.3321, -1.4985, 0.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, 4.8440, 0.6927, 1.3321, -1.4985, 0.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, -0.1560, 5.6927, 1.3321, -1.4985, 0.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, -0.1560, 0.6927, 6.3321, -1.4985, 0.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, -0.1560, 0.6927, 1.3321, 3.5015, 0.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, -0.1560, 0.6927, 1.3321, -1.4985, 5.6510, 0.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, -0.1560, 0.6927, 1.3321, -1.4985, 0.6510, 5.1489, 1.4966], [ 0.9892, -0.3591, -0.2757, -0.1560, 0.6927, 1.3321, -1.4985, 0.6510, 0.1489, 6.4966]], device=&#39;cuda:0&#39;)) . for img in learn.model.generator(alter_single_params).clamp(0,1): show_image(img) . Here we are slowing increasing the value of a single parameter, which slowly changes the output image in some way. There are other architectures that would have cleaner feature maps in the latent space than this one. . for i in range(10): img=learn.model.generator(rand+ones[:,0]*0.3*(i-5))[0].clamp(0,1) show_image(img) . def gen_loss(fake_pred, output, target): return fake_pred.mean() . def critic_loss(real_pred, fake_pred): return real_pred.mean() - fake_pred.mean() . This first module you will notice is AddChannels. This Module is simply responsible for simply making our noise into the correct shape for the following Conv Layer. Remember a ConvLayer expects a shape of BatchSize x Channels x Width x Height. We are going to reproduce this now. Later we will reproduce ConvTranspose using padding, and a regular Convolutional Layer. . learn.model.generator . Sequential( (0): AddChannels() (1): ConvLayer( (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ConvLayer( (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): ConvLayer( (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (4): ConvLayer( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (5): ConvLayer( (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (6): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (7): Tanh() ) . class AddChannels(Module): &quot;Add `n_dim` channels at the end of the input.&quot; def __init__(self, n_dim): self.n_dim=n_dim def forward(self, x): return x.view(*(list(x.shape)+[1]*self.n_dim)) . x=dls.one_batch()[0] n_dim=2 list(x.shape),list(x.shape)+[1]*n_dim . ([128, 100], [128, 100, 1, 1]) . x=AddChannels(2)(x) x.shape . torch.Size([128, 100, 1, 1]) . convT = nn.ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False) . Below is the values we are trying to match using our padding+convultion approach. . convT(x.cpu())[0,0,0:4,0:4] . tensor([[-0.0810, 0.0225, -0.0731, 0.0024], [-0.0440, -0.0085, -0.0398, 0.0384], [-0.0440, -0.0033, 0.0532, 0.1079], [ 0.0460, 0.0549, 0.0687, -0.0188]], grad_fn=&lt;SelectBackward&gt;) . Here we are pad every parameter of our input with a lot of zeroes, this will end up giving us a single specific weight from our weights. We are effectively multiply everything by a bunch of zeroes, so this is not how this tends to be done in practice. Though it helps to understand how we are doing this, because instead of a ConvTranspose a lot of the time we instead use Reflection Padding and a regular Convolutional Layer. . x=F.pad(x, (3, 3, 3, 3), &quot;constant&quot;, 0) print(x[0][0]) print(x.shape) . tensor([[ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, -1.0047, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device=&#39;cuda:0&#39;) torch.Size([128, 100, 7, 7]) . F.conv2d(x.cpu(),convT.weight.transpose(0, 1).flip(2).flip(3))[0,0,0:4,0:4] . tensor([[-0.0810, 0.0225, -0.0731, 0.0024], [-0.0440, -0.0085, -0.0398, 0.0384], [-0.0440, -0.0033, 0.0532, 0.1079], [ 0.0460, 0.0549, 0.0687, -0.0188]], grad_fn=&lt;SelectBackward&gt;) . As you see these are the same, showing that we reemplemented ConvTRanspose in a similar, albeit inefficient way. Though, this gives us the intuitive understanding of why we did it this way. . Further Reading . If you would like to read more about GANs, then I would invite you to take a look at DeOldify(https://github.com/jantic/DeOldify). You can also take a look at last year&#39;s fastai GAN lesson https://course.fast.ai/videos/?lesson=7 . export2html.notebook2html(fname=&#39;2020-06-11-Training-GANs.ipynb&#39;, dest=&#39;html/&#39;, template_file=&#39;fastpages.tpl&#39;,n_workers=1) . converting: 2020-06-11-Training-GANs.ipynb .",
            "url": "https://marii-moe.github.io/marii-blog/gan/image%20generation/2020/06/11/Training-GANs.html",
            "relUrl": "/gan/image%20generation/2020/06/11/Training-GANs.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Training in FP16",
            "content": "from nbdev import export2html . from fastai2.basics import * from fastai2.vision.all import * . path=untar_data(URLs.IMAGENETTE) . db=DataBlock((ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label,item_tfms=Resize(420),batch_tfms=aug_transforms(size=240)) . dls=db.dataloaders(path) . FP32 Training . We start with a normal fp32 trained model as a baseline. . learner=cnn_learner(dls,resnet50,pretrained=False) . learner.lr_find() . (0.0015848932787775993, 7.585775847473997e-07) . learner.fit_one_cycle(30,lr_max=0.00275) . epoch train_loss valid_loss time . 0 | 2.651336 | 2.056535 | 01:32 | . 1 | 2.355040 | 2.005668 | 01:33 | . 2 | 1.932189 | 1.904302 | 01:33 | . 3 | 1.597234 | 11.361675 | 01:33 | . 4 | 1.417671 | 1.946748 | 01:33 | . 5 | 1.845148 | 14.619100 | 01:33 | . 6 | 1.722217 | 3.752728 | 01:33 | . 7 | 1.463190 | 1.973663 | 01:34 | . 8 | 1.256870 | 1.894250 | 01:33 | . 9 | 1.120424 | 2.125210 | 01:34 | . 10 | 0.985170 | 1.701216 | 01:34 | . 11 | 0.905693 | 1.357060 | 01:34 | . 12 | 0.827493 | 0.917599 | 01:34 | . 13 | 0.760628 | 0.794275 | 01:34 | . 14 | 0.706049 | 0.966785 | 01:34 | . 15 | 0.672275 | 0.784713 | 01:34 | . 16 | 0.624744 | 1.014771 | 01:34 | . 17 | 0.555164 | 0.583922 | 01:34 | . 18 | 0.504771 | 0.766427 | 01:34 | . 19 | 0.486427 | 0.609142 | 01:34 | . 20 | 0.437780 | 0.533482 | 01:34 | . 21 | 0.411537 | 0.425947 | 01:34 | . 22 | 0.363749 | 0.550511 | 01:34 | . 23 | 0.354133 | 0.402780 | 01:34 | . 24 | 0.296108 | 0.394826 | 01:35 | . 25 | 0.288457 | 0.379434 | 01:34 | . 26 | 0.280953 | 0.379418 | 01:34 | . 27 | 0.264525 | 0.366601 | 01:34 | . 28 | 0.252890 | 0.366681 | 01:34 | . 29 | 0.244159 | 0.364107 | 01:34 | . learner.recorder.plot_loss(skip_start=20,with_valid=False) . del learner . FP16 . This is a purely trained fp16 model, and this isn&#39;t really done. . class MixedPrecision(Callback): &quot;Run training in mixed precision&quot; run_before = Recorder def __init__(self): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; def begin_batch(self): self.learn.xb = to_half(self.xb) def after_batch(self): self.learn.loss = to_float(self.learn.loss) . class ModelToHalf(Callback): &quot;Use with MixedPrecision callback (but it needs to run at the very beginning)&quot; run_before=TrainEvalCallback def begin_fit(self): self.learn.model = self.model.half() def after_fit(self): self.learn.model = self.model.float() #convert back to float, for saving and such . learner=cnn_learner(dls,resnet50,pretrained=False) learner.add_cbs((ModelToHalf(),MixedPrecision())) learner.fit_one_cycle(30,lr_max=0.00275) learner.recorder.plot_loss(skip_start=20) . epoch train_loss valid_loss time . 0 | 2.771560 | 2.216975 | 01:08 | . 1 | 2.476073 | 2.158066 | 01:05 | . 2 | 2.018307 | 1.633934 | 01:05 | . 3 | 1.684251 | 1.469444 | 01:05 | . 4 | 1.457907 | nan | 01:05 | . 5 | 1.428203 | 1.248842 | 01:05 | . 6 | 1.536437 | 3.168239 | 01:05 | . 7 | 1.225266 | 1.096525 | 01:05 | . 8 | 1.103817 | 1.346189 | 01:05 | . 9 | 0.969087 | 1.327698 | 01:05 | . 10 | 0.868427 | 1.027902 | 01:05 | . 11 | 0.785972 | 1.644904 | 01:04 | . 12 | 0.724989 | 1.882721 | 01:05 | . 13 | 0.677303 | 0.913675 | 01:05 | . 14 | 0.621354 | 0.907284 | 01:04 | . 15 | 0.583794 | 0.668342 | 01:05 | . 16 | 0.539824 | 1.777023 | 01:04 | . 17 | 0.486352 | 0.597984 | 01:04 | . 18 | 0.448464 | 1.210160 | 01:04 | . 19 | 0.410327 | 0.628322 | 01:04 | . 20 | 0.390925 | 0.589199 | 01:04 | . 21 | 0.358350 | 0.448581 | 01:05 | . 22 | 0.329511 | 0.430788 | 01:04 | . 23 | 0.292921 | 0.467403 | 01:05 | . 24 | 0.274056 | 0.421481 | 01:05 | . 25 | 0.237699 | 0.403993 | 01:05 | . 26 | 0.226854 | 0.395099 | 01:05 | . 27 | 0.231199 | 0.392145 | 01:05 | . 28 | 0.222327 | 0.391661 | 01:05 | . 29 | 0.220277 | 0.392155 | 01:05 | . del learner . FP16 with FP32 BatchNorm . We now use ModelToHalf from fastai2, the convert_network function converts it to a specfic data type without converting batchnorm. . del ModelToHalf #reimporting ModelToHalf from fastai2 from fastai2.vision.all import * . class MixedPrecision(Callback): &quot;Run training in mixed precision&quot; run_before = Recorder def __init__(self): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; def begin_batch(self): self.learn.xb = to_half(self.xb) def after_batch(self): self.learn.loss = to_float(self.learn.loss) . learner=cnn_learner(dls,resnet50,pretrained=False) learner.add_cbs((ModelToHalf(),MixedPrecision())) learner.fit_one_cycle(30,lr_max=0.00275) learner.recorder.plot_loss(skip_start=20) . epoch train_loss valid_loss time . 0 | 2.676094 | 2.062241 | 01:02 | . 1 | 2.394882 | 2.239057 | 01:02 | . 2 | 2.002832 | 1.627104 | 01:03 | . 3 | 1.757027 | 1.800131 | 01:02 | . 4 | 1.429366 | 3.083183 | 01:02 | . 5 | 1.479008 | 4.834050 | 01:02 | . 6 | 1.459492 | 1.715717 | 01:02 | . 7 | 1.335028 | 1.120237 | 01:02 | . 8 | 1.188040 | 1.130354 | 01:02 | . 9 | 0.991492 | 1.022801 | 01:02 | . 10 | 0.864592 | 1.437108 | 01:02 | . 11 | 0.796745 | 0.886712 | 01:02 | . 12 | 0.723405 | 0.980490 | 01:02 | . 13 | 0.682777 | 0.801665 | 01:02 | . 14 | 0.618106 | 0.826624 | 01:02 | . 15 | 0.580326 | 0.619600 | 01:02 | . 16 | 0.558647 | 0.959586 | 01:02 | . 17 | 0.504328 | 3.640037 | 01:02 | . 18 | 0.460288 | 0.511587 | 01:02 | . 19 | 0.419463 | 0.492790 | 01:03 | . 20 | 0.401901 | 0.803859 | 01:02 | . 21 | 0.355560 | 0.455122 | 01:02 | . 22 | 0.318891 | 0.428618 | 01:02 | . 23 | 0.290560 | 0.404963 | 01:02 | . 24 | 0.269655 | 0.456370 | 01:02 | . 25 | 0.242815 | 0.396282 | 01:03 | . 26 | 0.236340 | 0.376630 | 01:02 | . 27 | 0.233574 | 0.381232 | 01:02 | . 28 | 0.220733 | 0.376414 | 01:02 | . 29 | 0.223098 | 0.378601 | 01:02 | . del learner . FP16 with loss in fp32 . class MixedPrecision(Callback): &quot;Run training in mixed precision&quot; toward_end = True def __init__(self): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; def begin_batch(self): self.learn.xb = to_half(self.xb) def after_pred(self): self.learn.pred = to_float(self.pred) . learner=cnn_learner(dls,resnet50,pretrained=False) learner.add_cbs((ModelToHalf(),MixedPrecision())) learner.fit_one_cycle(30,lr_max=0.00275) learner.recorder.plot_loss(skip_start=20) . epoch train_loss valid_loss time . 0 | 2.656821 | 2.037143 | 01:02 | . 1 | 2.375797 | 1.916572 | 01:03 | . 2 | 2.028292 | 1.720190 | 01:03 | . 3 | 1.701117 | 1.807122 | 01:03 | . 4 | 1.499982 | 2.164554 | 01:03 | . 5 | 1.584262 | 1.594690 | 01:02 | . 6 | 1.310206 | 1.201177 | 01:02 | . 7 | 1.285866 | 6.326769 | 01:03 | . 8 | 1.195278 | 2.182448 | 01:02 | . 9 | 1.027358 | 1.015578 | 01:02 | . 10 | 0.937293 | 0.922560 | 01:02 | . 11 | 0.843483 | 1.100431 | 01:02 | . 12 | 0.765530 | 1.015288 | 01:02 | . 13 | 0.717502 | 0.840533 | 01:03 | . 14 | 0.686295 | 1.225734 | 01:02 | . 15 | 0.615597 | 0.854381 | 01:02 | . 16 | 0.556095 | 0.558067 | 01:02 | . 17 | 0.528304 | 1.195737 | 01:02 | . 18 | 0.495503 | 0.668504 | 01:02 | . 19 | 0.457330 | 0.541667 | 01:02 | . 20 | 0.418028 | 1.149197 | 01:02 | . 21 | 0.369291 | 0.504387 | 01:03 | . 22 | 0.338492 | 0.522376 | 01:02 | . 23 | 0.316736 | 0.404864 | 01:02 | . 24 | 0.277326 | 0.405616 | 01:02 | . 25 | 0.273004 | 0.402565 | 01:03 | . 26 | 0.244644 | 0.393408 | 01:03 | . 27 | 0.229975 | 0.391603 | 01:02 | . 28 | 0.224649 | 0.389079 | 01:02 | . 29 | 0.237047 | 0.389761 | 01:02 | . del learner del MixedPrecision . fp16 with loss in fp32, with loss scale without fp32 accumulate . class MixedPrecision(Callback): &quot;Run training in mixed precision&quot; toward_end=True def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2., scale_wait=500, clip=None): assert torch.backends.cudnn.enabled, &quot;Mixed precision training requires cudnn.&quot; self.flat_master,self.dynamic,self.max_loss_scale = flat_master,dynamic,max_loss_scale self.div_factor,self.scale_wait,self.clip = div_factor,scale_wait,clip self.loss_scale = max_loss_scale if dynamic else loss_scale def begin_fit(self): if self.learn.opt is None: self.learn.create_opt() self.model_pgs,_ = get_master(self.opt, self.flat_master) self.old_pgs = self.opt.param_groups #Changes the optimizer so that the optimization step is done in FP32. if self.dynamic: self.count = 0 def begin_batch(self): self.learn.xb = to_half(self.xb) def after_pred(self): self.learn.pred = to_float(self.pred) def after_loss(self): if self.training: self.learn.loss *= self.loss_scale def after_backward(self): self.learn.loss /= self.loss_scale #To record the real loss #First, check for an overflow if self.dynamic and grad_overflow(self.model_pgs): self.loss_scale /= self.div_factor self.model.zero_grad() raise CancelBatchException() #skip step and zero_grad for params in self.model_pgs: for param in params: if param.grad is not None: param.grad.div_(self.loss_scale) #Check if it&#39;s been long enough without overflow if self.clip is not None: for group in self.model_pgs: nn.utils.clip_grad_norm_(group, self.clip) if self.dynamic: self.count += 1 if self.count == self.scale_wait: self.count = 0 self.loss_scale *= self.div_factor def after_step(self): self.model.zero_grad() #Zero the gradients of the model manually (optimizer disconnected) def after_fit(self): self.learn.opt.param_groups = self.old_pgs delattr(self, &quot;model_pgs&quot;) delattr(self, &quot;old_pgs&quot;) . learner=cnn_learner(dls,resnet50,pretrained=False) learner.add_cbs((ModelToHalf(),MixedPrecision())) learner.fit_one_cycle(30,lr_max=0.00275) learner.recorder.plot_loss(skip_start=20) . epoch train_loss valid_loss time . 0 | 2.708762 | 2.261719 | 01:08 | . 1 | 2.401522 | 1.939942 | 01:10 | . 2 | 1.998625 | 2.204348 | 01:10 | . 3 | 1.600047 | 1.737028 | 01:10 | . 4 | 1.380643 | 3.546641 | 01:10 | . 5 | 1.532201 | 3.255549 | 01:10 | . 6 | 1.318069 | 1.603604 | 01:10 | . 7 | 1.233026 | 1.262678 | 01:10 | . 8 | 1.016523 | 1.241237 | 01:10 | . 9 | 0.907380 | 2.354774 | 01:11 | . 10 | 0.842147 | 0.953867 | 01:10 | . 11 | 0.756705 | 1.135527 | 01:10 | . 12 | 0.729484 | 0.750674 | 01:11 | . 13 | 0.700462 | 1.289216 | 01:10 | . 14 | 0.617139 | 0.583479 | 01:11 | . 15 | 0.570849 | 0.880843 | 01:10 | . 16 | 0.551235 | 0.582794 | 01:10 | . 17 | 0.480748 | 0.767345 | 01:10 | . 18 | 0.460275 | 0.978343 | 01:10 | . 19 | 0.414565 | 0.457135 | 01:10 | . 20 | 0.378115 | 0.449951 | 01:10 | . 21 | 0.337399 | 0.486601 | 01:10 | . 22 | 0.307479 | 0.414679 | 01:10 | . 23 | 0.298396 | 0.416613 | 01:10 | . 24 | 0.269521 | 0.396884 | 01:10 | . 25 | 0.233148 | 0.387787 | 01:10 | . 26 | 0.243302 | 0.372072 | 01:10 | . 27 | 0.218694 | 0.371554 | 01:10 | . 28 | 0.218254 | 0.371993 | 01:10 | . 29 | 0.218056 | 0.371956 | 01:10 | . del learner del MixedPrecision . FP16 with loss_scale . from fastai2.vision.all import * . learner=cnn_learner(dls,resnet50,pretrained=False) learner.to_fp16() learner.fit_one_cycle(30,lr_max=0.00275) learner.recorder.plot_loss(skip_start=20) . epoch train_loss valid_loss time . 0 | 2.757428 | 2.081527 | 01:12 | . 1 | 2.353672 | 2.234412 | 01:14 | . 2 | 2.012591 | 1.837813 | 01:13 | . 3 | 1.635887 | 3.204813 | 01:14 | . 4 | 1.531814 | 1.518298 | 01:13 | . 5 | 1.446371 | nan | 01:14 | . 6 | 1.498608 | 2.455172 | 01:13 | . 7 | 1.350126 | 1.248328 | 01:13 | . 8 | 1.250748 | 6.087388 | 01:13 | . 9 | 1.085607 | 5.156849 | 01:13 | . 10 | 1.042411 | 1.595870 | 01:13 | . 11 | 0.934609 | 1.178143 | 01:13 | . 12 | 0.805411 | 0.984007 | 01:14 | . 13 | 0.760592 | 2.332953 | 01:13 | . 14 | 0.708662 | 1.113873 | 01:13 | . 15 | 0.641280 | 0.628776 | 01:14 | . 16 | 0.588315 | 2.150546 | 01:13 | . 17 | 0.537062 | 1.012078 | 01:13 | . 18 | 0.507948 | 0.634026 | 01:14 | . 19 | 0.488397 | 0.671309 | 01:13 | . 20 | 0.448709 | 0.809253 | 01:14 | . 21 | 0.393053 | 0.442282 | 01:13 | . 22 | 0.365035 | 0.425639 | 01:14 | . 23 | 0.327412 | 0.451561 | 01:13 | . 24 | 0.328089 | 0.423867 | 01:13 | . 25 | 0.279865 | 0.396011 | 01:14 | . 26 | 0.261059 | 0.378210 | 01:13 | . 27 | 0.260404 | 0.366170 | 01:14 | . 28 | 0.247177 | 0.362599 | 01:14 | . 29 | 0.262104 | 0.363247 | 01:14 | . Reduced range in fp16⁴. Insert Image . As seen about there is a limited range to fp16. fp16 overflow is when your gradients are too big too fit within fp16. Underflow is when fp16 gradients are so close to 0 that they are simply rounded to 0. Overflow and underflow can happen in fp32 as well, but isn&#39;t nearly as big of a problem because fp32 is able to &quot;hold&quot; more information. Loss scaling is multiplying our loss by as large a value as possible,which effectively is multiplied by our gradients to keep them within its representable range. This avoids underflow by increasing the gradients, but we need to make sure it is not big enough that it causes an overflow. One technicality to loss scaling is calculating the loss itself in fp32, because calculating a loss includes division and low precision division is very in-precise leading to unstable gradients/training. Another technicality is that the gradients have to be divided by the loss scale, to keep the magnitudes consistent with a fp32-bit model, otherwise this would be very similar to simply increasing the learning rate by the loss scale! But, wait a minute… if we divide by the loss scale won&#39;t we just run into the underflow issue from before? Your right, we have to have a fp32 copy of the weights as well, so that we can avoid this problem. We copy the scaled gradients over to an fp32 model and then divide by the loss scale there. We then continue the optimization step in fp32. . Batch Size . Batch size is imporant in fp16 training. I found that, the very first convolution layer was very unstable in my training. Increasing the batch size tends to smooth out this first layer&#39;s gradients as there are more inputs to the layer. . Limiting Maximum Loss and Differential Loss Functions . I found that is was very important to effectively cap the maximum loss and minimum loss. This becaomes a problem if your model is expected to have very large loss values such as values greater than a magnitude of 100. Generally the loss is positive and relatively small, though in the case of Wasserstein (https://arxiv.org/abs/1506.05439) loss the loss can take a negative value. For large maximal values I found that differential losses can be very large, especially if your loss functions look something like (1000 x loss1 + loss2). In the case of differential loss functions, I found that I was able to get satisfactory results simply by changing the function to 1000 x tanh(loss1)+loss2. I expect gradient clipping would also be very helpful, but I have not yet experimented with that myself. . Normalization . In my experience I found normalization to be very important in making sure an architecture performed well in fp16 training. Some architectures with very little normalization I had constant issues with overflowing or underflowing gradients. Introducing Similar normalization layers to those already present in the model gave me similar results to the fp32 models. . Contact Me . I am looking for a Job . Linked-In Profile: https://www.linkedin.com/in/molly-beavers-651025118/ Source Code(WIP): https://github.com/marii-moe/selfie2anime . export2html.notebook2html(fname=&#39;2020-05-11_FP16.ipynb&#39;, dest=&#39;html/&#39;, template_file=&#39;fastpages.tpl&#39;,n_workers=1) . converting: 2020-05-11_FP16.ipynb .",
            "url": "https://marii-moe.github.io/marii-blog/ugatit/fp16/gan/image%20generation/image%20to%20image/2020/06/11/FP16.html",
            "relUrl": "/ugatit/fp16/gan/image%20generation/image%20to%20image/2020/06/11/FP16.html",
            "date": " • Jun 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://marii-moe.github.io/marii-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://marii-moe.github.io/marii-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marii-moe.github.io/marii-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}